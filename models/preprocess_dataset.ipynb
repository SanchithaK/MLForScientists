{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install required libraries\n"
      ],
      "metadata": {
        "id": "pXZq01NQsfwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scanpy\n",
        "!pip install optuna\n",
        "!pip install igraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTcFJFEIu7FK",
        "outputId": "e25d2e97-8971-4f60-8c93-acdee24ede58"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scanpy in /usr/local/lib/python3.11/dist-packages (1.11.1)\n",
            "Requirement already satisfied: anndata>=0.8 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.11.4)\n",
            "Requirement already satisfied: h5py>=3.7 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.13.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.4.2)\n",
            "Requirement already satisfied: legacy-api-wrap>=1.4 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.10.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.4.2)\n",
            "Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from scanpy) (2.0.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scanpy) (24.2)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (2.2.2)\n",
            "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.0.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.13)\n",
            "Requirement already satisfied: scikit-learn<1.6.0,>=1.1 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.15.2)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.13.2)\n",
            "Requirement already satisfied: session-info2 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.1.2)\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.14.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.13.2)\n",
            "Requirement already satisfied: umap-learn!=0.5.0,>=0.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.7)\n",
            "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /usr/local/lib/python3.11/dist-packages (from anndata>=0.8->scanpy) (1.11.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.57->scanpy) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->scanpy) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->scanpy) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6.0,>=1.1->scanpy) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7->scanpy) (1.17.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
            "Requirement already satisfied: igraph in /usr/local/lib/python3.11/dist-packages (0.11.8)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from igraph) (1.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load required packages"
      ],
      "metadata": {
        "id": "B-WUoVDPuohK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scanpy as sc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sklearn\n",
        "import optuna\n",
        "sc.settings.verbosity = 1\n",
        "print(f'sklearn_version:{sklearn.__version__}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqyVyV7YukVq",
        "outputId": "3bdee1b4-8b3d-4012-ba89-b468d08d28b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sklearn_version:1.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_type = \"cancer\" # neuro,cancer\n",
        "dataset_folder = os.path.join(os.getcwd(),\"data\")\n",
        "dataset_path  = os.path.join(dataset_folder,f'{dataset_type}.h5ad')\n",
        "dataset_subpath = os.path.join(dataset_folder,f'{dataset_type}_hvg.h5ad')\n",
        "dataset_pandas_path = os.path.join(dataset_folder,f'{dataset_type}_hvg.csv')\n",
        "dataset_preaggregated_path = os.path.join(dataset_folder,f'{dataset_type}_preaggregated_.csv')\n",
        "dataset_aggregated_path = os.path.join(dataset_folder,f'{dataset_type}_aggregated.csv')\n",
        "dataset_train_path = os.path.join(dataset_folder,f'train_{dataset_type}.csv')\n",
        "dataset_test_path = os.path.join(dataset_folder,f'test_{dataset_type}.csv')\n",
        "DOWNLOAD= True\n",
        "RANDOM_SEED = 38\n",
        "dataset_links = {\"neuro\": \"https://datasets.cellxgene.cziscience.com/2808a16d-64c5-451b-91a5-c1a2d9f270d5.h5ad \", \"cancer\": \"https://datasets.cellxgene.cziscience.com/76c942bd-45c3-47ec-b290-1e695ec9c177.h5ad\"}\n",
        "if not os.path.isdir(dataset_folder):\n",
        "  print(f'creating dataset_folder={dataset_folder}')\n",
        "  os.makedirs(dataset_folder,exist_ok=True)\n",
        "\n",
        "\n",
        "if not os.path.isfile(dataset_path) and DOWNLOAD:\n",
        "  dataset_link = dataset_links[dataset_type]\n",
        "  print(f'downloading dataset from link={dataset_link}.....')\n",
        "  print(f\"...and saving to {dataset_path}....\")\n",
        "  command = f\"wget {dataset_link}  -O {dataset_path}\"\n",
        "  print(f'running command:{command}')\n",
        "  os.system(command)\n",
        "\n"
      ],
      "metadata": {
        "id": "90tfi7SvAC7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68c06eeb-6038-44d8-ff4b-8e688cc46fde"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading dataset from link=https://datasets.cellxgene.cziscience.com/76c942bd-45c3-47ec-b290-1e695ec9c177.h5ad.....\n",
            "...and saving to /content/data/cancer.h5ad....\n",
            "running command:wget https://datasets.cellxgene.cziscience.com/76c942bd-45c3-47ec-b290-1e695ec9c177.h5ad  -O /content/data/cancer.h5ad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset preprocessing"
      ],
      "metadata": {
        "id": "PCiz5VG5wcr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions\n"
      ],
      "metadata": {
        "id": "TyS2A_H_knde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_categorical_columns(data = None, columns=[],category='ohe'):\n",
        "  \"\"\"\n",
        "  This function encodes columns in data to either one hot encoding or label\n",
        "  encoding\n",
        "  Args:\n",
        "  data (pd.Dataframe): preaggregated data e.g gene_exp_df\n",
        "  columns (list of str) : columns to encode\n",
        "  category (str): type of encoding e.g ohe or le\n",
        "  returns:\n",
        "  pd.DataFrame\n",
        "  \"\"\"\n",
        "  cols_before = set(data.columns)\n",
        "  if category == 'ohe':\n",
        "    data = pd.get_dummies(data, columns=columns)\n",
        "  elif category == 'le':\n",
        "    new_columns = [f'{column}_encoded' for column in columns]\n",
        "    data[new_columns] = data[columns].apply(lambda x: pd.factorize(x)[0])\n",
        "    return data,new_columns\n",
        "  else:\n",
        "    raise Exception(f'invalid category={category}')\n",
        "  encoded_columns = list(set(data.columns).difference(cols_before))\n",
        "  data[encoded_columns] = data[encoded_columns].astype(np.int64)\n",
        "  return data,encoded_columns\n",
        "\n",
        "def compute_drop_columns(data=None,columns = [],null_threshold = 0.33,n=1000):\n",
        "  \"\"\"\n",
        "  This function drops columns that have null percentage exceeding threshod\n",
        "  encoding\n",
        "  Args:\n",
        "  data (pd.Dataframe): preaggregated data e.g gene_exp_df\n",
        "  columns (list of str) : columns to consider for drop\n",
        "  null_threshold (float): nul percentage to consider when dropping column\n",
        "  n (int): sample size of dataframe\n",
        "  returns:\n",
        "  drop_columns : list of str\n",
        "  \"\"\"\n",
        "  drop_columns = []\n",
        "  for col in columns:\n",
        "    nan_count = data[col].isna().sum()\n",
        "    nan_percentage = nan_count/n\n",
        "    if nan_percentage > 0:\n",
        "      print(f'col={col} null_perc={nan_percentage}')\n",
        "    if nan_percentage > null_threshold:\n",
        "      print(f'dropping col={col}')\n",
        "      drop_columns.append(col)\n",
        "  return drop_columns\n",
        "\n",
        "def handle_missing_data(data=None,numerical_columns=[],categorical_columns=[]):\n",
        "  \"\"\"\n",
        "  This function replaces Nan with mean for numerical columns\n",
        "  and with mode for categorical columns\n",
        "  Args:\n",
        "  data (pd.Dataframe): preaggregated data e.g gene_exp_df\n",
        "  numerical_columns (list of str) : numerical columns to fill with mean\n",
        "  categorical_columns (list of str) : categorical columns to fill with mode\n",
        "  returns:\n",
        "  pd.DataFrame\n",
        "  \"\"\"\n",
        "  for col in numerical_columns:\n",
        "    m = data[col].mean()\n",
        "    data[col].fillna(m,inplace=True)\n",
        "  for col in categorical_columns:\n",
        "    m = data[col].mode()\n",
        "    data[col].fillna(m,inplace=True)\n",
        "  return data"
      ],
      "metadata": {
        "id": "9rQmaPUtkuGW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Subset dataset"
      ],
      "metadata": {
        "id": "FjVTNN9Q1FCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOAD_COMPLETE_DATASET = False # High RAM needed ~50gb\n",
        "#need to run LOAD_COMPLETE_DATASET=True to run LOAD_PREAGGREGATED_DATASET=True and LOAD_AGGREGATED_DATASET=True\n",
        "LOAD_PREAGGREGATED_DATASET = False  and (not LOAD_COMPLETE_DATASET)# LOAD_COMPLETE_DATASET needs to be False and False if high RAM available\n",
        "LOAD_AGGREGATED_DATASET = False and (not LOAD_PREAGGREGATED_DATASET)#LOAD_PREAGGREGATED_DATASET needs to be False and True if NOT high RAM available\n",
        "# can run LOAD_TRAIN_TEST_DATASET independently of the rest as long as we have data/ folder in same directory as notebook with appropriate files\n",
        "LOAD_TRAIN_TEST_DATASET = True and (not LOAD_AGGREGATED_DATASET) # LOAD_AGGREGATED_DATASET needs to be False and True if NOT high RAM available\n"
      ],
      "metadata": {
        "id": "N27v1lhJ42Qf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_COMPLETE_DATASET :\n",
        "  print(f'reading complete dataset at: {dataset_path}')\n",
        "  adata = sc.read_h5ad(dataset_path)\n",
        "  vars = list(adata.var_names)\n",
        "  non_gene_vars = [var for var in vars if 'ENSG' not in var]\n",
        "  print(f'non_gene_vars={non_gene_vars}')\n",
        "  #ordering of filtering matters\n",
        "  #we should filter by cells and then genes\n",
        "  sc.pp.filter_cells(adata, min_genes=150,inplace=True) # keep cells with gene exp in atleast 150 genes\n",
        "  sc.pp.filter_genes(adata, min_cells=15,inplace=True)  # keep genes expressed in â‰¥15 cells\n",
        "  remaining_genes = list(adata.var_names)\n",
        "  filtered_genes = set(vars).difference(set(remaining_genes))\n",
        "  print('filtered_genes:',len(filtered_genes))\n",
        "  print('remaining_genes:',len(remaining_genes))\n",
        "\n",
        "  #total count normalization\n",
        "  sc.pp.normalize_total(adata,target_sum=1e6)\n",
        "  # log scale data to ensure smoother distribution\n",
        "  sc.pp.log1p(adata)\n",
        "\n",
        "  #plot highly expressive genes\n",
        "  sc.pl.highest_expr_genes(adata, n_top=50)\n",
        "\n",
        "  #dimensionality reduction\n",
        "  sc.pp.highly_variable_genes(adata, n_top_genes=3000, flavor='seurat',inplace=True)\n",
        "  adata  = adata[:, adata.var['highly_variable']]\n",
        "\n",
        "  #use pca\n",
        "  sc.pp.pca(adata, svd_solver=\"arpack\", use_highly_variable=True)\n",
        "  sc.pl.pca_scatter(adata, color=\"disease\")\n",
        "  sc.pl.pca_scatter(adata, color=\"tissue\")\n",
        "  sc.pl.pca_scatter(adata, color=\"cell_type\")\n",
        "\n",
        "  #save the smaller scanpy file with highly variable genes only (hvg)\n",
        "  save_subset = True\n",
        "  adata  = adata[:, adata.var['highly_variable']]\n",
        "  if save_subset:\n",
        "    print(f'saving dataset to dataset_subpath={dataset_subpath}')\n",
        "    adata.write_h5ad(dataset_subpath)\n",
        "  #specify columns for our dataset\n",
        "  metadata_columns = ['donor_id']\n",
        "  ohe_columns = ['sex','cell_type','tissue'] #keeping for aggregation purposes\n",
        "  label_column = ['disease']\n",
        "  le_columns = label_column\n",
        "  categorical_columns = ohe_columns\n",
        "  numerical_columns = [col for col in list(adata.var_names) if 'ENSG' in col]\n",
        "  #create dataframe to facilitate modeling and analysis\n",
        "  gene_expr_df = pd.DataFrame(adata.X.toarray(), index=adata.obs['donor_id'], columns= numerical_columns)\n",
        "  for col in categorical_columns + label_column:\n",
        "    gene_expr_df[col] = adata.obs[col].values\n",
        "  print(f'saving dataset to dataset_preaggregated_path={dataset_preaggregated_path}')\n",
        "  gene_expr_df.to_csv(dataset_preaggregated_path,index=True)\n",
        "  print(f'saving dataset from dataset_preaggregated_path={dataset_preaggregated_path}')\n",
        "  #there is a bug in scanpy so its important to load file as a csv directly from pandas\n",
        "  gene_expr_df = pd.read_csv(dataset_preaggregated_path)\n",
        "  num_samples = gene_expr_df.shape[0]\n",
        "\n",
        "  drop_cols = compute_drop_columns(data=gene_expr_df,columns = numerical_columns + categorical_columns,null_threshold = 0.33,n=num_samples)\n",
        "  if len(drop_cols):\n",
        "    print(f\"drop_cols:{drop_cols}\")\n",
        "    gene_expr_df.drop(drop_cols,axis=1,inplace=True)\n",
        "  #aggregating dataset to reduce sparsity issue and allow for compute efficiency\n",
        "  group_cols = ['donor_id','disease']\n",
        "  print(f'categorical_columns:{categorical_columns} numerical_columns:{numerical_columns}')\n",
        "  agg_expr_df = gene_expr_df.groupby(group_cols).mean(numerical_columns).reset_index()\n",
        "  before = agg_expr_df.shape[0]\n",
        "  nan_columns_agg = agg_expr_df.isna().sum()[agg_expr_df.isna().sum() > 0].index\n",
        "  print(f'nan_columns:{nan_columns_agg}')\n",
        "  print(f'saving aggregated dataset to {dataset_aggregated_path}')\n",
        "  agg_expr_df.to_csv(dataset_aggregated_path,index=False)\n",
        "  agg_expr_df\n",
        "elif LOAD_PREAGGREGATED_DATASET:\n",
        "  print(f'reading dataset from dataset_preaggregated_path={dataset_preaggregated_path}')\n",
        "  gene_expr_df = pd.read_csv(dataset_preaggregated_path)\n",
        "  metadata_columns = ['donor_id']\n",
        "  ohe_columns = ['sex','cell_type','tissue'] #keeping for aggregation purposes\n",
        "  label_column = ['disease']\n",
        "  le_columns = label_column\n",
        "  categorical_columns = ohe_columns + le_columns\n",
        "  numerical_columns = [col for col in list(gene_expr_df) if 'ENSG' in col]\n",
        "  num_samples = gene_expr_df.shape[0]\n",
        "  drop_cols = compute_drop_columns(data=gene_expr_df,columns = numerical_columns + categorical_columns,null_threshold = 0.33,n=num_samples)\n",
        "  if len(drop_cols):\n",
        "    print(f\"drop_cols:{drop_cols}\")\n",
        "    gene_expr_df.drop(drop_cols,axis=1,inplace=True)\n",
        "\n",
        "  #aggregating dataset to reduce sparsity issue and allow for compute efficiency\n",
        "  group_cols = ['donor_id','disease']\n",
        "  print(f'categorical_columns:{categorical_columns} numerical_columns:{numerical_columns}')\n",
        "  agg_expr_df = gene_expr_df.groupby(group_cols).mean(numerical_columns).reset_index()\n",
        "  before = agg_expr_df.shape[0]\n",
        "  nan_columns_agg = agg_expr_df.isna().sum()[agg_expr_df.isna().sum() > 0].index\n",
        "  print(f'nan_columns:{nan_columns_agg}')\n",
        "  print(f'saving aggregated dataset to {dataset_aggregated_path}')\n",
        "  assert all([True if 'ENSG' in col else False for col in numerical_columns ])\n",
        "  agg_expr_df.to_csv(dataset_aggregated_path,index=False)\n",
        "  agg_expr_df\n",
        "elif LOAD_AGGREGATED_DATASET:\n",
        "  print(f'reading aggregated dataset from {dataset_aggregated_path}')\n",
        "  #load aggregated dataset\n",
        "  agg_expr_df = pd.read_csv(dataset_aggregated_path)\n",
        "  #set dataset specific respective columns\n",
        "  metadata_columns = ['donor_id']\n",
        "  ohe_columns = ['sex','cell_type','tissue'] #keeping for aggregation purposes\n",
        "  label_column = ['disease']\n",
        "  le_columns = label_column\n",
        "  categorical_columns = ohe_columns + le_columns\n",
        "  #ensg are gene ids\n",
        "  numerical_columns = [col for col in list(agg_expr_df) if 'ENSG' in col]\n",
        "  assert all([True if 'ENSG' in col else False for col in numerical_columns ])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Iikyb5ICwhOT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Dataset"
      ],
      "metadata": {
        "id": "9OxJxRKI7fxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import StratifiedGroupKFold,GroupKFold,GroupShuffleSplit\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "miZrOWZZxFb7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stratified split"
      ],
      "metadata": {
        "id": "9VvBYtHUECS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not LOAD_TRAIN_TEST_DATASET:\n",
        "  print('running train/test splits...')\n",
        "  ohe_columns = ['sex','cell_type','tissue'] #keeping for aggregation purposes\n",
        "  if dataset_type == 'cancer':\n",
        "    ignore_cols = ['cell_type','sex','tissue','development_stage']\n",
        "    ohe_columns = []\n",
        "    print(f'ignoring columns={ignore_cols} for {dataset_type} dataset ')\n",
        "    agg_expr_df.drop(ignore_cols,axis=1,inplace=True)\n",
        "    ohe_columns = list(set(ohe_columns).difference(set(ignore_cols)))\n",
        "  le_columns = ['disease']\n",
        "  if len(ohe_columns):\n",
        "    agg_expr_df,ohe_columns = process_categorical_columns(data = agg_expr_df,columns=ohe_columns,category='ohe' )\n",
        "  agg_expr_df,le_columns_new = process_categorical_columns(data = agg_expr_df,columns=le_columns,category='le' )\n",
        "  categorical_columns = ohe_columns\n",
        "  feature_cols = list(set(agg_expr_df.columns).intersection(set(categorical_columns + numerical_columns)))\n",
        "  categorical_columns = list(set(feature_cols).difference(set(numerical_columns)))\n",
        "  label_columns = ['disease','disease_encoded']\n",
        "  label_column = 'disease_encoded'\n",
        "  assert len(set(feature_cols).intersection(set(metadata_columns))) == 0\n",
        "  assert len(feature_cols) == len(categorical_columns + numerical_columns)\n",
        "  print(f'categorical_cols:{len(categorical_columns)}')\n",
        "  print(f'numerical_cols:{len(numerical_columns)}')\n",
        "  print(f'feature_cols:{len(feature_cols)}')\n",
        "  print(f'le_cols:{len(le_columns)}')\n",
        "  print(f'le_cols_new:{len(le_columns_new)}')\n",
        "\n",
        "  agg_expr_df[feature_cols + label_columns]\n",
        "  # Use StratifiedGroupKFold for donor-aware stratified splitting\n",
        "  splitter = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
        "  label_column = 'disease_encoded'\n",
        "  metadata_column = 'donor_id'\n",
        "  group_col = metadata_column\n",
        "  y = agg_expr_df[label_column]\n",
        "  groups = agg_expr_df[metadata_column]\n",
        "\n",
        "  # Grab first split only\n",
        "  for train_idx, test_idx in splitter.split(agg_expr_df, agg_expr_df[label_column], groups):\n",
        "      df_train = agg_expr_df.iloc[train_idx].copy()\n",
        "      df_test = agg_expr_df.iloc[test_idx].copy()\n",
        "      break\n",
        "\n",
        "  # Sanity checks\n",
        "  assert not set(df_train[group_col]) & set(df_test[group_col]), \"Donor overlap detected!\"\n",
        "\n",
        "  # Check label distribution\n",
        "  train_dist = df_train[label_column].value_counts(normalize=True).rename(\"train %\")\n",
        "  test_dist = df_test[label_column].value_counts(normalize=True).rename(\"test %\")\n",
        "  print(\"Disease label proportions (train vs test):\")\n",
        "  print(pd.concat([train_dist, test_dist], axis=1))\n",
        "  print(\"Train shape:\", df_train.shape)\n",
        "  print(\"Test shape:\", df_test.shape)\n",
        "\n",
        "\n",
        "  # -------------------------\n",
        "  # STANDARDIZATION (Z-scoring only gene expression features)\n",
        "  # -------------------------\n",
        "\n",
        "  # Compute mean/std from training data\n",
        "  mu = df_train[numerical_columns].mean()\n",
        "  sd = df_train[numerical_columns].std().replace(0, 1)\n",
        "\n",
        "  # Apply standardization\n",
        "  df_train[numerical_columns] = (df_train[numerical_columns] - mu) / sd\n",
        "  df_test[numerical_columns] = (df_test[numerical_columns] - mu) / sd\n",
        "\n",
        "\n",
        "  # Save standardized splits\n",
        "  print(f'saving train to {dataset_train_path}')\n",
        "  print(f'saving test to {dataset_test_path}')\n",
        "  df_train.to_csv(dataset_train_path, index=False)\n",
        "  df_test.to_csv(dataset_test_path, index=False)\n",
        "  print(\"Standardized gene expression features saved.\")\n",
        "else:\n",
        "  # Load standardized splits\n",
        "  print(f'Loading train from {dataset_train_path}')\n",
        "  print(f'Loading test from {dataset_test_path}')\n",
        "\n",
        "\n",
        "\n",
        "  df_train = pd.read_csv(dataset_train_path)\n",
        "  df_test = pd.read_csv(dataset_test_path)\n",
        "\n",
        "\n",
        "  train_groups = df_train.donor_id.unique()\n",
        "  test_groups = df_test.donor_id.unique()\n",
        "\n",
        "  assert len(set(df_train.columns).intersection(set(df_test.columns))) == len(df_train.columns)\n",
        "  #need to ohe train+test combined\n",
        "  agg_expr_df = pd.concat([df_train,df_test],axis=0)\n",
        "  if dataset_type == 'cancer':\n",
        "    ignore_cols = ['cell_type','sex','tissue','development_stage']\n",
        "    ohe_columns = []\n",
        "    print(f'ignoring columns={ignore_cols} for {dataset_type} dataset ')\n",
        "    agg_expr_df.drop(ignore_cols,axis=1,inplace=True)\n",
        "  else:\n",
        "    ohe_columns = ['sex','cell_type','tissue']\n",
        "\n",
        "  le_columns = ['disease']\n",
        "  if len(ohe_columns):\n",
        "    agg_expr_df,ohe_columns = process_categorical_columns(data = agg_expr_df,columns=ohe_columns,category='ohe' )\n",
        "  agg_expr_df,le_columns_new = process_categorical_columns(data = agg_expr_df,columns=le_columns,category='le' )\n",
        "  categorical_columns = ohe_columns\n",
        "  numerical_columns = [col for col in df_train.columns if 'ENSG' in col]\n",
        "  label_column = ['disease_encoded']\n",
        "  label_columns = ['disease','disease_encoded']\n",
        "  metadata_columns = ['donor_id']\n",
        "  feature_cols = list(set(agg_expr_df.columns).intersection(set(categorical_columns + numerical_columns)))\n",
        "  categorical_columns = list(set(feature_cols).difference(set(numerical_columns)))\n",
        "  label_columns = ['disease','disease_encoded']\n",
        "  label_column = 'disease_encoded'\n",
        "  assert len(set(feature_cols).intersection(set(metadata_columns))) == 0\n",
        "  assert len(feature_cols) == len(categorical_columns + numerical_columns)\n",
        "  df_train , df_test = agg_expr_df[agg_expr_df.donor_id.isin(train_groups)],agg_expr_df[agg_expr_df.donor_id.isin(test_groups)]\n",
        "\n",
        "\n",
        "\n",
        "train_groups = df_train['donor_id']\n",
        "X_train,X_test = df_train[feature_cols],df_test[feature_cols]\n",
        "y_train,y_test = df_train[label_columns],df_test[label_columns]\n",
        "\n",
        "\n",
        "print('train:',X_train.shape,y_train.shape)\n",
        "print('test:',X_test.shape,y_test.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-xhi6tV1OrZ",
        "outputId": "6b43fb0f-5250-4a2d-b9af-2d50f747d897"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading train from /content/data/train_cancer.csv\n",
            "Loading test from /content/data/test_cancer.csv\n",
            "ignoring columns=['cell_type', 'sex', 'tissue', 'development_stage'] for cancer dataset \n",
            "train: (839, 1500) (839, 2)\n",
            "test: (227, 1500) (227, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(y_train[label_column].unique())\n",
        "num_features = len(feature_cols)\n",
        "print(f'num_featiures={num_features}')\n",
        "print(f'num_classes={num_classes}')\n",
        "assert len(feature_cols) == X_train.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qKNaL0wJOrb",
        "outputId": "88ce0e1f-c36e-4624-f8a7-88cadb4e9d93"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_featiures=1500\n",
            "num_classes=8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if len(list(set(X_train.columns).intersection(set(metadata_columns)))):\n",
        "  print(f'dropping columns={metadata_columns} from train')\n",
        "  X_train.drop(metadata_columns,axis=1,inplace=True)\n",
        "  print(X_train.shape,y_train.shape)\n",
        "\n",
        "if len(list(set(X_test.columns).intersection(set(metadata_columns)))):\n",
        "  print(f'dropping columns={metadata_columns} from test')\n",
        "  X_test.drop(metadata_columns,axis=1,inplace=True)\n",
        "  print(X_test.shape,y_test.shape)"
      ],
      "metadata": {
        "id": "ld3GDm9aKiJK"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}